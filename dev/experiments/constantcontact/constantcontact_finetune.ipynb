{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf2zD6_24JV7"
      },
      "source": [
        "##**Connected to OracleDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roirjnzHmG6h",
        "outputId": "669e36c7-1baf-4f3d-972a-d582e396f3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'oracle-instantclient*-basiclite-*.rpm': No such file or directory\n",
            "--2023-12-22 19:33:35--  https://yum.oracle.com/repo/OracleLinux/OL7/oracle/instantclient/x86_64/getPackage/oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm\n",
            "Resolving yum.oracle.com (yum.oracle.com)... 23.222.201.187, 2600:1408:c400:1682::2a7d, 2600:1408:c400:168e::2a7d\n",
            "Connecting to yum.oracle.com (yum.oracle.com)|23.222.201.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28553496 (27M) [application/x-rpm]\n",
            "Saving to: ‘oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm’\n",
            "\n",
            "oracle-instantclien 100%[===================>]  27.23M   125MB/s    in 0.2s    \n",
            "\n",
            "2023-12-22 19:33:35 (125 MB/s) - ‘oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm’ saved [28553496/28553496]\n",
            "\n",
            "ls: cannot access 'oracle-instantclient*-sqlplus-*.rpm': No such file or directory\n",
            "--2023-12-22 19:33:35--  https://yum.oracle.com/repo/OracleLinux/OL7/oracle/instantclient/x86_64/getPackage/oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm\n",
            "Resolving yum.oracle.com (yum.oracle.com)... 23.222.201.187, 2600:1408:c400:168e::2a7d, 2600:1408:c400:1682::2a7d\n",
            "Connecting to yum.oracle.com (yum.oracle.com)|23.222.201.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 702792 (686K) [application/x-rpm]\n",
            "Saving to: ‘oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm’\n",
            "\n",
            "oracle-instantclien 100%[===================>] 686.32K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-12-22 19:33:35 (66.1 MB/s) - ‘oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm’ saved [702792/702792]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! ls -l oracle-instantclient*-basiclite-*.rpm || wget https://yum.oracle.com/repo/OracleLinux/OL7/oracle/instantclient/x86_64/getPackage/oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm\n",
        "! ls -l oracle-instantclient*-sqlplus-*.rpm   || wget https://yum.oracle.com/repo/OracleLinux/OL7/oracle/instantclient/x86_64/getPackage/oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPE38DjRmJNS",
        "outputId": "c5cfffaf-1c16-47c7-c10c-172c8794f27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cx_Oracle\n",
            "  Downloading cx_Oracle-8.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (892 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/892.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/892.6 kB\u001b[0m \u001b[31m806.2 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/892.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/892.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cx_Oracle\n",
            "Successfully installed cx_Oracle-8.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install cx_Oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3VIOlQcoqAV",
        "outputId": "9c558fe0-45ef-48c7-de83-031ce04e942b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libaio1 is already the newest version (0.3.112-13build1).\n",
            "libaio1 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autopoint cpio debhelper debugedit dh-autoreconf dh-strip-nondeterminism dwz\n",
            "  gettext gettext-base intltool-debian libarchive-cpio-perl\n",
            "  libarchive-zip-perl libdebhelper-perl libfile-stripnondeterminism-perl\n",
            "  libfsverity0 liblua5.3-0 libmail-sendmail-perl librpm9 librpmbuild9\n",
            "  librpmio9 librpmsign9 libsub-override-perl libsys-hostname-long-perl libtool\n",
            "  po-debconf rpm rpm-common rpm2cpio\n",
            "Suggested packages:\n",
            "  lintian libarchive1 dh-make gettext-doc libasprintf-dev libgettextpo-dev\n",
            "  libtool-doc gcj-jdk libmail-box-perl elfutils rpmlint rpm-i18n\n",
            "The following NEW packages will be installed:\n",
            "  alien autopoint cpio debhelper debugedit dh-autoreconf\n",
            "  dh-strip-nondeterminism dwz gettext gettext-base intltool-debian\n",
            "  libarchive-cpio-perl libarchive-zip-perl libdebhelper-perl\n",
            "  libfile-stripnondeterminism-perl libfsverity0 liblua5.3-0\n",
            "  libmail-sendmail-perl librpm9 librpmbuild9 librpmio9 librpmsign9\n",
            "  libsub-override-perl libsys-hostname-long-perl libtool po-debconf rpm\n",
            "  rpm-common rpm2cpio\n",
            "0 upgraded, 29 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 3,928 kB of archives.\n",
            "After this operation, 11.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 cpio amd64 2.13+dfsg-7 [83.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 gettext-base amd64 0.21-4ubuntu4 [37.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 autopoint all 0.21-4ubuntu4 [422 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdebhelper-perl all 13.6ubuntu1 [67.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtool all 2.4.6-15build2 [164 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 dh-autoreconf all 20 [16.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libarchive-zip-perl all 1.68-1 [90.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsub-override-perl all 0.09-2 [9,532 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfile-stripnondeterminism-perl all 1.13.0-1 [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 dh-strip-nondeterminism all 1.13.0-1 [5,344 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 debugedit amd64 1:5.0-4build1 [47.2 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 dwz amd64 0.14-1build2 [105 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 gettext amd64 0.21-4ubuntu4 [868 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 intltool-debian all 0.35.0+20060710.5 [24.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 po-debconf all 1.0.21+nmu1 [233 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 debhelper all 13.6ubuntu1 [923 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblua5.3-0 amd64 5.3.6-1build1 [140 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librpmio9 amd64 4.17.0+dfsg1-4build1 [93.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librpm9 amd64 4.17.0+dfsg1-4build1 [189 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librpmbuild9 amd64 4.17.0+dfsg1-4build1 [80.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfsverity0 amd64 1.4-1~exp1build1 [10.4 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librpmsign9 amd64 4.17.0+dfsg1-4build1 [11.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 rpm-common amd64 4.17.0+dfsg1-4build1 [33.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 rpm2cpio amd64 4.17.0+dfsg1-4build1 [9,882 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 rpm amd64 4.17.0+dfsg1-4build1 [151 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 alien all 8.95.5 [50.9 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libarchive-cpio-perl all 0.10-1.1 [9,928 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsys-hostname-long-perl all 1.5-2 [11.5 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmail-sendmail-perl all 0.80-1.1 [22.7 kB]\n",
            "Fetched 3,928 kB in 0s (11.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 29.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cpio.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cpio_2.13+dfsg-7_amd64.deb ...\n",
            "Unpacking cpio (2.13+dfsg-7) ...\n",
            "Selecting previously unselected package gettext-base.\n",
            "Preparing to unpack .../01-gettext-base_0.21-4ubuntu4_amd64.deb ...\n",
            "Unpacking gettext-base (0.21-4ubuntu4) ...\n",
            "Selecting previously unselected package autopoint.\n",
            "Preparing to unpack .../02-autopoint_0.21-4ubuntu4_all.deb ...\n",
            "Unpacking autopoint (0.21-4ubuntu4) ...\n",
            "Selecting previously unselected package libdebhelper-perl.\n",
            "Preparing to unpack .../03-libdebhelper-perl_13.6ubuntu1_all.deb ...\n",
            "Unpacking libdebhelper-perl (13.6ubuntu1) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../04-libtool_2.4.6-15build2_all.deb ...\n",
            "Unpacking libtool (2.4.6-15build2) ...\n",
            "Selecting previously unselected package dh-autoreconf.\n",
            "Preparing to unpack .../05-dh-autoreconf_20_all.deb ...\n",
            "Unpacking dh-autoreconf (20) ...\n",
            "Selecting previously unselected package libarchive-zip-perl.\n",
            "Preparing to unpack .../06-libarchive-zip-perl_1.68-1_all.deb ...\n",
            "Unpacking libarchive-zip-perl (1.68-1) ...\n",
            "Selecting previously unselected package libsub-override-perl.\n",
            "Preparing to unpack .../07-libsub-override-perl_0.09-2_all.deb ...\n",
            "Unpacking libsub-override-perl (0.09-2) ...\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\n",
            "Preparing to unpack .../08-libfile-stripnondeterminism-perl_1.13.0-1_all.deb ...\n",
            "Unpacking libfile-stripnondeterminism-perl (1.13.0-1) ...\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\n",
            "Preparing to unpack .../09-dh-strip-nondeterminism_1.13.0-1_all.deb ...\n",
            "Unpacking dh-strip-nondeterminism (1.13.0-1) ...\n",
            "Selecting previously unselected package debugedit.\n",
            "Preparing to unpack .../10-debugedit_1%3a5.0-4build1_amd64.deb ...\n",
            "Unpacking debugedit (1:5.0-4build1) ...\n",
            "Selecting previously unselected package dwz.\n",
            "Preparing to unpack .../11-dwz_0.14-1build2_amd64.deb ...\n",
            "Unpacking dwz (0.14-1build2) ...\n",
            "Selecting previously unselected package gettext.\n",
            "Preparing to unpack .../12-gettext_0.21-4ubuntu4_amd64.deb ...\n",
            "Unpacking gettext (0.21-4ubuntu4) ...\n",
            "Selecting previously unselected package intltool-debian.\n",
            "Preparing to unpack .../13-intltool-debian_0.35.0+20060710.5_all.deb ...\n",
            "Unpacking intltool-debian (0.35.0+20060710.5) ...\n",
            "Selecting previously unselected package po-debconf.\n",
            "Preparing to unpack .../14-po-debconf_1.0.21+nmu1_all.deb ...\n",
            "Unpacking po-debconf (1.0.21+nmu1) ...\n",
            "Selecting previously unselected package debhelper.\n",
            "Preparing to unpack .../15-debhelper_13.6ubuntu1_all.deb ...\n",
            "Unpacking debhelper (13.6ubuntu1) ...\n",
            "Selecting previously unselected package liblua5.3-0:amd64.\n",
            "Preparing to unpack .../16-liblua5.3-0_5.3.6-1build1_amd64.deb ...\n",
            "Unpacking liblua5.3-0:amd64 (5.3.6-1build1) ...\n",
            "Selecting previously unselected package librpmio9.\n",
            "Preparing to unpack .../17-librpmio9_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking librpmio9 (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package librpm9.\n",
            "Preparing to unpack .../18-librpm9_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking librpm9 (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package librpmbuild9.\n",
            "Preparing to unpack .../19-librpmbuild9_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking librpmbuild9 (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package libfsverity0:amd64.\n",
            "Preparing to unpack .../20-libfsverity0_1.4-1~exp1build1_amd64.deb ...\n",
            "Unpacking libfsverity0:amd64 (1.4-1~exp1build1) ...\n",
            "Selecting previously unselected package librpmsign9.\n",
            "Preparing to unpack .../21-librpmsign9_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking librpmsign9 (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package rpm-common.\n",
            "Preparing to unpack .../22-rpm-common_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking rpm-common (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package rpm2cpio.\n",
            "Preparing to unpack .../23-rpm2cpio_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking rpm2cpio (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package rpm.\n",
            "Preparing to unpack .../24-rpm_4.17.0+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking rpm (4.17.0+dfsg1-4build1) ...\n",
            "Selecting previously unselected package alien.\n",
            "Preparing to unpack .../25-alien_8.95.5_all.deb ...\n",
            "Unpacking alien (8.95.5) ...\n",
            "Selecting previously unselected package libarchive-cpio-perl.\n",
            "Preparing to unpack .../26-libarchive-cpio-perl_0.10-1.1_all.deb ...\n",
            "Unpacking libarchive-cpio-perl (0.10-1.1) ...\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\n",
            "Preparing to unpack .../27-libsys-hostname-long-perl_1.5-2_all.deb ...\n",
            "Unpacking libsys-hostname-long-perl (1.5-2) ...\n",
            "Selecting previously unselected package libmail-sendmail-perl.\n",
            "Preparing to unpack .../28-libmail-sendmail-perl_0.80-1.1_all.deb ...\n",
            "Unpacking libmail-sendmail-perl (0.80-1.1) ...\n",
            "Setting up cpio (2.13+dfsg-7) ...\n",
            "update-alternatives: using /bin/mt-gnu to provide /bin/mt (mt) in auto mode\n",
            "Setting up libtool (2.4.6-15build2) ...\n",
            "Setting up libarchive-zip-perl (1.68-1) ...\n",
            "Setting up libdebhelper-perl (13.6ubuntu1) ...\n",
            "Setting up gettext-base (0.21-4ubuntu4) ...\n",
            "Setting up libfsverity0:amd64 (1.4-1~exp1build1) ...\n",
            "Setting up autopoint (0.21-4ubuntu4) ...\n",
            "Setting up dwz (0.14-1build2) ...\n",
            "Setting up libarchive-cpio-perl (0.10-1.1) ...\n",
            "Setting up liblua5.3-0:amd64 (5.3.6-1build1) ...\n",
            "Setting up debugedit (1:5.0-4build1) ...\n",
            "Setting up libsub-override-perl (0.09-2) ...\n",
            "Setting up libsys-hostname-long-perl (1.5-2) ...\n",
            "Setting up librpmio9 (4.17.0+dfsg1-4build1) ...\n",
            "Setting up libfile-stripnondeterminism-perl (1.13.0-1) ...\n",
            "Setting up librpm9 (4.17.0+dfsg1-4build1) ...\n",
            "Setting up gettext (0.21-4ubuntu4) ...\n",
            "Setting up rpm-common (4.17.0+dfsg1-4build1) ...\n",
            "Setting up intltool-debian (0.35.0+20060710.5) ...\n",
            "Setting up dh-autoreconf (20) ...\n",
            "Setting up libmail-sendmail-perl (0.80-1.1) ...\n",
            "Setting up librpmbuild9 (4.17.0+dfsg1-4build1) ...\n",
            "Setting up dh-strip-nondeterminism (1.13.0-1) ...\n",
            "Setting up librpmsign9 (4.17.0+dfsg1-4build1) ...\n",
            "Setting up rpm2cpio (4.17.0+dfsg1-4build1) ...\n",
            "Setting up po-debconf (1.0.21+nmu1) ...\n",
            "Setting up rpm (4.17.0+dfsg1-4build1) ...\n",
            "Setting up debhelper (13.6ubuntu1) ...\n",
            "Setting up alien (8.95.5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ],
      "source": [
        "! sudo apt-get install alien libaio1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M56Da9Nhoyh9",
        "outputId": "20de3cb4-a38f-4b05-c302-78fe56b51429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "Warning: Skipping conversion of scripts in package oracle-instantclient19.3-basiclite: postinst postrm\n",
            "Warning: Use the --scripts parameter to include the scripts.\n",
            "warning: oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "\tdpkg --no-force-overwrite -i oracle-instantclient19.3-basiclite_19.3.0.0.0-2_amd64.deb\n",
            "Selecting previously unselected package oracle-instantclient19.3-basiclite.\n",
            "(Reading database ... 122857 files and directories currently installed.)\n",
            "Preparing to unpack oracle-instantclient19.3-basiclite_19.3.0.0.0-2_amd64.deb ...\n",
            "Unpacking oracle-instantclient19.3-basiclite (19.3.0.0.0-2) ...\n",
            "Setting up oracle-instantclient19.3-basiclite (19.3.0.0.0-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! sudo alien -i oracle-instantclient19.3-basiclite-19.3.0.0.0-1.x86_64.rpm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIe98PEdpBTn",
        "outputId": "c79e42fb-906d-45ba-9454-1e17960da3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "warning: oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\n",
            "\tdpkg --no-force-overwrite -i oracle-instantclient19.3-sqlplus_19.3.0.0.0-2_amd64.deb\n",
            "Selecting previously unselected package oracle-instantclient19.3-sqlplus.\n",
            "(Reading database ... 122894 files and directories currently installed.)\n",
            "Preparing to unpack oracle-instantclient19.3-sqlplus_19.3.0.0.0-2_amd64.deb ...\n",
            "Unpacking oracle-instantclient19.3-sqlplus (19.3.0.0.0-2) ...\n",
            "Setting up oracle-instantclient19.3-sqlplus (19.3.0.0.0-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! sudo alien -i --scripts oracle-instantclient19.3-sqlplus-19.3.0.0.0-1.x86_64.rpm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brjxyvd2RVk",
        "outputId": "867a9aea-83ee-41b2-e6c9-8b72d4f2f638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1005 - subjectline\n",
            "1005 - subjectline\n",
            "123 - image_targeting\n",
            "123 - image_targeting\n",
            "1 - image_targeting\n",
            "1 - image_targeting\n",
            "1 - image_targeting\n",
            "2 - image_targeting\n",
            "2 - image_targeting\n",
            "2 - image_targeting\n",
            "3 - image_targeting\n",
            "3 - image_targeting\n",
            "3 - image_targeting\n",
            "1005 - subjectline\n",
            "123 - image_targeting\n"
          ]
        }
      ],
      "source": [
        "import cx_Oracle\n",
        "import sys\n",
        "\n",
        "# Create a DSN (Data Source Name) for the connection\n",
        "dsn_tns = cx_Oracle.makedsn('73.222.84.xxx', '1521', service_name='freepdb1')\n",
        "\n",
        "# Connect to the Oracle database using the provided user, password, and DSN\n",
        "conn = cx_Oracle.connect(user=r'hr', password='oracle', dsn=dsn_tns)\n",
        "\n",
        "# Create a cursor to execute SQL queries\n",
        "c = conn.cursor()  # Fixed the syntax issue here by removing the trailing '/'\n",
        "\n",
        "# Execute the SQL query\n",
        "c.execute('select * from ML_PROCESSING_STATUS')\n",
        "\n",
        "# Print the results of the query\n",
        "for row in c:\n",
        "    print(row[0], '-', row[3])  # Print the first and fourth columns of each row\n",
        "\n",
        "# Close the connection (this line is commented out, so it won't be executed)\n",
        "# conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAXVWFQDmbXj",
        "outputId": "b9ab21ab-2992-4f8f-ad8f-439e3258e044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sqlplus is /usr/bin/sqlplus\n",
            "lrwxrwxrwx 1 root root 39 Dec 22 19:35 /usr/bin/sqlplus -> ../lib/oracle/19.3/client64/bin/sqlplus\n"
          ]
        }
      ],
      "source": [
        "! type sqlplus\n",
        "! ls -l /usr/bin/sqlplus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGapL1wcpqa0"
      },
      "outputs": [],
      "source": [
        "#!sqlplus /nolog <<<'exit'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMi04a0wlfp5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcpxVjxYlgqW"
      },
      "source": [
        "##**Import data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-My5w_glhKs",
        "outputId": "daaa3221-6c02-4073-cab0-1eca4d799016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "#Get the necessary libraries ready\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import cx_Oracle\n",
        "import sys\n",
        "!pip install transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KP7LesTUln3Z",
        "outputId": "895c3071-ba8a-4778-9aea-f4eb8ebb0946"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5f80aed-e609-4629-b4c1-6eb7b3f307e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5f80aed-e609-4629-b4c1-6eb7b3f307e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving transition_data.csv to transition_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Upload and read the CSV data\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"transition_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kEI9aeYlrVw"
      },
      "outputs": [],
      "source": [
        "# Convert string timestamps to datetime objects\n",
        "timestamp_columns = [\n",
        "    'ORDER_PURCHASE_TIMESTAMP',\n",
        "    'ORDER_APPROVED_AT',\n",
        "    'ORDER_DELIVERED_CARRIER_DATE',\n",
        "    'ORDER_DELIVERED_CUSTOMER_DATE',\n",
        "    'ORDER_ESTIMATED_DELIVERY_DATE'\n",
        "]\n",
        "for column in timestamp_columns:\n",
        "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmwfqN09gh1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4kK9ui6l18g"
      },
      "source": [
        "##**Database Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kansJ_4BlxAZ"
      },
      "outputs": [],
      "source": [
        "# Establish connection\n",
        "dsn_tns = cx_Oracle.makedsn('73.222.84.xxx', '1521', service_name='freepdb1')\n",
        "conn = cx_Oracle.connect(user=r'hr', password='oracle', dsn=dsn_tns)\n",
        "c = conn.cursor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIrC0yPDl40U",
        "outputId": "42a7fe8c-be83-4908-ba96-da57ba59265f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transition_data table dropped.\n"
          ]
        }
      ],
      "source": [
        "# Check and drop table if exists\n",
        "c.execute(\"SELECT COUNT(*) FROM user_tables WHERE table_name = 'TRANSITION_DATA'\")\n",
        "# If the count is 1, the table exists; if 0, it doesn't.\n",
        "if c.fetchone()[0] == 1:\n",
        "    c.execute(\"DROP TABLE transition_data\")\n",
        "    print(\"transition_data table dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKdEf4aAl7Of"
      },
      "outputs": [],
      "source": [
        "# Create the table\n",
        "create_table_query = \"\"\"\n",
        "CREATE TABLE transition_data (\n",
        "    ACCOUNT_ID VARCHAR(50),\n",
        "    ORDER_ID VARCHAR(255),\n",
        "    CUSTOMER_ID VARCHAR(255),\n",
        "    ORDER_STATUS VARCHAR(100),\n",
        "    ORDER_PURCHASE_TIMESTAMP TIMESTAMP,\n",
        "    ORDER_APPROVED_AT TIMESTAMP,\n",
        "    ORDER_DELIVERED_CARRIER_DATE TIMESTAMP,\n",
        "    ORDER_DELIVERED_CUSTOMER_DATE TIMESTAMP,\n",
        "    ORDER_ESTIMATED_DELIVERY_DATE TIMESTAMP\n",
        ")\n",
        "\"\"\"\n",
        "c.execute(create_table_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "v1iQ0Cmhl9em",
        "outputId": "d49963ed-4369-4155-b60c-72bd3b7cae92"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-21c9f6be6834>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Update data (set the values in specific columns to NULL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Insert data\n",
        "insert_query = \"\"\"\n",
        "INSERT INTO transition_data (ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP, ORDER_APPROVED_AT, ORDER_DELIVERED_CARRIER_DATE, ORDER_DELIVERED_CUSTOMER_DATE, ORDER_ESTIMATED_DELIVERY_DATE)\n",
        "VALUES (:1, :2, :3, :4, :5, :6, :7, :8, :9)\n",
        "\"\"\"\n",
        "for _, row in df.iterrows():\n",
        "    c.execute(insert_query, tuple(row))\n",
        "\n",
        "# Update data (set the values in specific columns to NULL)\n",
        "update_query = \"\"\"\n",
        "UPDATE transition_data\n",
        "SET\n",
        "    ORDER_APPROVED_AT = NULL,\n",
        "    ORDER_DELIVERED_CARRIER_DATE = NULL,\n",
        "    ORDER_DELIVERED_CUSTOMER_DATE = NULL,\n",
        "    ORDER_ESTIMATED_DELIVERY_DATE = NULL\n",
        "\"\"\"\n",
        "c.execute(update_query)\n",
        "\n",
        "# Add, rename, and delete columns\n",
        "alter_table_add_query = \"\"\"\n",
        "ALTER TABLE transition_data\n",
        "ADD DUMMY_COLUMN VARCHAR2(255) DEFAULT 'dummy_value'\n",
        "\"\"\"\n",
        "alter_table_rename_query = \"\"\"\n",
        "ALTER TABLE transition_data\n",
        "RENAME COLUMN DUMMY_COLUMN TO APPROPRIATE_DESTINATION\n",
        "\"\"\"\n",
        "alter_table_drop_query = \"\"\"\n",
        "ALTER TABLE transition_data\n",
        "DROP COLUMN APPROPRIATE_DESTINATION\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWM50FTumoC2"
      },
      "outputs": [],
      "source": [
        "c.execute(alter_table_add_query)\n",
        "c.execute(alter_table_rename_query)\n",
        "c.execute(alter_table_drop_query)\n",
        "\n",
        "#conn.commit()\n",
        "#c.close()\n",
        "#conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52PV-Qs0rMua"
      },
      "source": [
        "###test for rename and delete columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rsnfEtirOcD"
      },
      "outputs": [],
      "source": [
        "# Ask the user for the desired action\n",
        "action = input(\"Would you like to delete or rename a column? (Enter 'delete' or 'rename'): \")\n",
        "\n",
        "if action == \"delete\":\n",
        "    # Ask the user for the name of the column to delete\n",
        "    column_to_delete = input(\"Enter the name of the column you'd like to delete: \")\n",
        "\n",
        "    # SQL to drop the specified column from the transition_data table\n",
        "    drop_column_query = f\"ALTER TABLE transition_data DROP COLUMN {column_to_delete}\"\n",
        "\n",
        "    # Execute the ALTER TABLE query to drop the column\n",
        "    c.execute(drop_column_query)\n",
        "    print(f\"{column_to_delete} has been deleted from transition_data table.\")\n",
        "\n",
        "elif action == \"rename\":\n",
        "    # Ask the user for the names of the columns\n",
        "    column_to_rename = input(\"Enter the name of the column you'd like to rename: \")\n",
        "    new_column_name = input(f\"Enter the new name for {column_to_rename}: \")\n",
        "\n",
        "    # SQL to rename the specified column in the transition_data table\n",
        "    rename_column_query = f\"ALTER TABLE transition_data RENAME COLUMN {column_to_rename} TO {new_column_name}\"\n",
        "\n",
        "    # Execute the ALTER TABLE query to rename the column\n",
        "    c.execute(rename_column_query)\n",
        "    print(f\"{column_to_rename} has been renamed to {new_column_name} in transition_data table.\")\n",
        "\n",
        "else:\n",
        "    print(\"Invalid action. Please enter either 'delete' or 'rename'.\")\n",
        "\n",
        "# Commit the changes to the database\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor (but keep your connection open if you need further operations)\n",
        "c.close()\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTzaNKzm2Pg"
      },
      "source": [
        "##**Work with GPT3.5 turbo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unOjrD-ENjdF",
        "outputId": "db7315e3-2aa5-4841-e44d-c30855605d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkRn0UDLOJ0q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'xxx'  # Replace 'YOUR_API_KEY' with your actual API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z44XPV15ve9Z"
      },
      "source": [
        "###First interaction with GPT-3.5 Turbo:\n",
        "\n",
        "Here, a conversation is initiated with GPT-3.5 Turbo. The system message sets the assistant's behavior. The user message asks the assistant for a SQL query. The response is stored in the response variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "YOTxjgQZOtPz",
        "outputId": "d36d0d6f-c739-4f78-c37d-d3b7cbe696c4"
      },
      "outputs": [
        {
          "ename": "APIRemovedInV1",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4e146e1b1802>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   messages=[\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a helpful assistant.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Can you provide me a sql query, which I want to drop the transition_data table first if its exist in the oracleDB, I've connected the oracleDB in colab, so I want to using python to opreate it \"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mproxied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proxied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide me a sql query, which I want to drop the transition_data table first if its exist in the oracleDB, I've connected the oracleDB in colab, so I want to using python to opreate it \"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfr0abUmRmTD"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQSyQbdeRp9C"
      },
      "outputs": [],
      "source": [
        "#This extracts the output from the API response.\n",
        "response[\"choices\"][0][\"message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1z2yzHDSM1o"
      },
      "outputs": [],
      "source": [
        " #Setting up Messages for Another Query:\n",
        " messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide me with a sql query, which I want to do drop the transition_data table first if its exist in the oracleDB\"},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBuAkGZkJiZN"
      },
      "outputs": [],
      "source": [
        "messages.append(response[\"choices\"][0][\"message\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqGBXqv-JkV4"
      },
      "outputs": [],
      "source": [
        "#Adding Another User Message to the Conversation:\n",
        "message={\"role\":\"user\",\"content\":\"Can you provide sql query to delete the data from specific column in oracleDB \"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPbqyAViJ6Qm"
      },
      "outputs": [],
      "source": [
        "messages.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s81_PHOrJ8vT"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6UoYqtwt0s"
      },
      "source": [
        "###Second Interaction with GPT-3.5 Turbo:\n",
        "\n",
        "Another conversation is initiated with GPT-3.5 Turbo using the updated messages list. The response contains the assistant's reply to the new user message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdpxKfpZJ-Ui"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0aOrwJ2KEhV"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRL-d9Diw63P"
      },
      "source": [
        "Here, a loop is initiated for continuous interactions with GPT-3.5 Turbo. The user can type their questions, and the assistant will reply. The loop continues indefinitely until it's manually interrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub8by_a1KGB5"
      },
      "outputs": [],
      "source": [
        "  #Continuous Interaction Loop with the Assistant:\n",
        "    while True:\n",
        "    text=input()\n",
        "    message={\"role\":\"user\",\"content\":text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=messages )\n",
        "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "    messages.append(response[\"choices\"][0][\"message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTytr-foDplk"
      },
      "source": [
        "##**Work with GPT3.5 turbo**\n",
        "\n",
        "2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJZLKrA3KLQH",
        "outputId": "b97f931e-ce32-4e66-b748-6dcd72e36378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.2-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m174.1/220.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.2\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the openai package\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmSCg8xNDkUK"
      },
      "outputs": [],
      "source": [
        "# 2. Import necessary modules\n",
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR82bkw5Dl4Z"
      },
      "outputs": [],
      "source": [
        "openai.api_key = 'xxx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "sE71eC0SkwhP",
        "outputId": "a45f7263-6e21-42d1-9272-995eb67fffa5"
      },
      "outputs": [
        {
          "ename": "APIRemovedInV1",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7b2cf308ee76>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4. Initial interaction with GPT-3.5 Turbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   messages=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a helpful assistant.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mproxied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proxied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "# 4. Initial interaction with GPT-3.5 Turbo\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide me a sql query, which I want to drop the transition_data table first if its exist in the oracleDB, I've connected the oracleDB in colab, so I want to use python to operate it\"},\n",
        "    ]\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzjQ3hJLk3tF"
      },
      "outputs": [],
      "source": [
        "# 5. Setup messages for another query\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide me with a sql query, which I want to drop the transition_data table first if its exist in the oracleDB\"},\n",
        "    response[\"choices\"][0][\"message\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OurQ_RFlEaJ"
      },
      "outputs": [],
      "source": [
        "# 6. Add another user message\n",
        "message = {\"role\":\"user\", \"content\":\"Can you provide a sql query to delete the data from a specific column in oracleDB\"}\n",
        "messages.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qNGa2hTlGh4"
      },
      "outputs": [],
      "source": [
        "# 7. Second interaction with GPT-3.5 Turbo\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R76e-KNxlISH"
      },
      "outputs": [],
      "source": [
        "# 8. Continuous interaction loop\n",
        "while True:\n",
        "    text = input()\n",
        "    message = {\"role\": \"user\", \"content\": text}\n",
        "    messages.append(message)\n",
        "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
        "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "    messages.append(response[\"choices\"][0][\"message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us2LbLV7vXy0"
      },
      "source": [
        "##**Fine-tuning a GPT-3 model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESoy2zlRgEH"
      },
      "source": [
        "Installing Necessary Libraries:\n",
        "\n",
        "- Upgrade and install the openai library.\n",
        "\n",
        "- Install requests for making API calls.\n",
        "\n",
        "Setting Up the Environment:\n",
        "\n",
        "\n",
        "- Import necessary Python modules (json, requests, time, files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMWjKF16iHF8",
        "outputId": "3f05b6cd-3692-4dd5-c27b-f760ec0a683c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "# Install and upgrade the OpenAI library\n",
        "!pip install --upgrade openai\n",
        "!pip install requests\n",
        "\n",
        "# Import the necessary modules\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z0D8JqsRoWP"
      },
      "source": [
        "\n",
        "Set the OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-2TIjfAiTrU"
      },
      "outputs": [],
      "source": [
        "# Set OpenAI API key\n",
        "api_key = 'xxx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cid0922oR4Aa"
      },
      "source": [
        "Preparing the Data:\n",
        "\n",
        "- Clear any existing data in the data list.\n",
        "\n",
        "- Populating the 'data' list with SQL query prompts and completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6gGT301AImx",
        "outputId": "07c9d356-f181-44f7-9103-540f3626e7db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of data entries: 50\n"
          ]
        }
      ],
      "source": [
        "#Clear the existing 'data' dataset\n",
        "data = []\n",
        "\n",
        "#Populating the 'data' list with SQL query prompts and completions\n",
        "data = [ # Each dictionary in the list represents a prompt-completion pair for SQL queries\n",
        "\n",
        "    {\n",
        "        \"prompt\": \"Insert a new order into the transition_data table.\",\n",
        "        \"completion\": \"INSERT INTO transition_data (ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP) VALUES ('123', '1140337857101', 'c3acec69-0bfd-11ee-a611-fa163e638ddc', 'EMAIL_SEND', '2023-06-28T19:31:00.000Z');\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Update the ORDER_STATUS to 'EMAIL_OPEN' for a specific order in the transition_data table.\",\n",
        "        \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'EMAIL_OPEN' WHERE ORDER_ID = '1140337857100';\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Select all orders with the status 'EMAIL_OPEN' from the transition_data table.\",\n",
        "        \"completion\": \"SELECT * FROM transition_data WHERE ORDER_STATUS = 'EMAIL_OPEN';\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Delete an order from the transition_data table using ORDER_ID.\",\n",
        "        \"completion\": \"DELETE FROM transition_data WHERE ORDER_ID = '1140314740354';\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Count the number of orders for each CUSTOMER_ID in the transition_data table.\",\n",
        "        \"completion\": \"SELECT CUSTOMER_ID, COUNT(*) as total_orders FROM transition_data GROUP BY CUSTOMER_ID;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Create an index on the CUSTOMER_ID column in the transition_data table to improve query performance.\",\n",
        "        \"completion\": \"CREATE INDEX idx_customer_id ON transition_data(CUSTOMER_ID);\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"List all unique CUSTOMER_IDs from the transition_data table.\",\n",
        "        \"completion\": \"SELECT DISTINCT CUSTOMER_ID FROM transition_data;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Calculate the total number of 'EMAIL_SEND' and 'EMAIL_OPEN' statuses in the transition_data table.\",\n",
        "        \"completion\": \"SELECT ORDER_STATUS, COUNT(*) as status_count FROM transition_data WHERE ORDER_STATUS IN ('EMAIL_SEND', 'EMAIL_OPEN') GROUP BY ORDER_STATUS;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Alter the transition_data table to add a column for order amount.\",\n",
        "        \"completion\": \"ALTER TABLE transition_data ADD COLUMN order_amount DECIMAL(10,2);\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Create a view to show the latest order for each customer from the transition_data table.\",\n",
        "        \"completion\": \"CREATE VIEW latest_customer_orders AS SELECT CUSTOMER_ID, MAX(ORDER_PURCHASE_TIMESTAMP) as latest_order FROM transition_data GROUP BY CUSTOMER_ID;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Find the earliest ORDER_PURCHASE_TIMESTAMP in the transition_data table.\",\n",
        "        \"completion\": \"SELECT MIN(ORDER_PURCHASE_TIMESTAMP) as earliest_order_time FROM transition_data;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Update the ORDER_APPROVED_AT timestamp to the current time for a specific order in the transition_data table.\",\n",
        "        \"completion\": \"UPDATE transition_data SET ORDER_APPROVED_AT = CURRENT_TIMESTAMP WHERE ORDER_ID = '1140327698389';\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Select orders from the transition_data table where ORDER_DELIVERED_CUSTOMER_DATE is null.\",\n",
        "        \"completion\": \"SELECT * FROM transition_data WHERE ORDER_DELIVERED_CUSTOMER_DATE IS NULL;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Create a trigger to log changes when the ORDER_STATUS column is updated in the transition_data table.\",\n",
        "        \"completion\": \"CREATE TRIGGER log_order_status_change AFTER UPDATE ON transition_data FOR EACH ROW WHEN OLD.ORDER_STATUS != NEW.ORDER_STATUS BEGIN INSERT INTO order_status_log (order_id, previous_status, new_status) VALUES (NEW.ORDER_ID, OLD.ORDER_STATUS, NEW.ORDER_STATUS); END;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a query to find the average time between ORDER_PURCHASE_TIMESTAMP and ORDER_DELIVERED_CUSTOMER_DATE for delivered orders.\",\n",
        "        \"completion\": \"SELECT AVG(ORDER_DELIVERED_CUSTOMER_DATE - ORDER_PURCHASE_TIMESTAMP) as avg_delivery_time FROM transition_data WHERE ORDER_STATUS = 'Delivered';\\n\"\n",
        "    },\n",
        "    {\n",
        "    \"prompt\": \"Alter the transition_data table to add a new column for tracking the order source.\",\n",
        "    \"completion\": \"ALTER TABLE transition_data ADD COLUMN order_source VARCHAR(100);\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Select all orders in the transition_data table made after June 25, 2023.\",\n",
        "    \"completion\": \"SELECT * FROM transition_data WHERE ORDER_PURCHASE_TIMESTAMP > '2023-06-25';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Update the transition_data table to set ORDER_STATUS to 'Cancelled' for orders not approved within 2 days.\",\n",
        "    \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'Cancelled' WHERE ORDER_APPROVED_AT IS NULL AND ORDER_PURCHASE_TIMESTAMP < CURRENT_DATE - INTERVAL '2 days';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a stored procedure to insert a new order into the transition_data table.\",\n",
        "    \"completion\": \"CREATE PROCEDURE InsertNewOrder(IN p_account_id VARCHAR(50), IN p_order_id VARCHAR(255), IN p_customer_id VARCHAR(255), IN p_order_status VARCHAR(100), IN p_order_purchase_timestamp TIMESTAMP) BEGIN INSERT INTO transition_data (ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP) VALUES (p_account_id, p_order_id, p_customer_id, p_order_status, p_order_purchase_timestamp); END;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Generate a report of the total number of orders each day from the transition_data table.\",\n",
        "    \"completion\": \"SELECT DATE(ORDER_PURCHASE_TIMESTAMP) as order_date, COUNT(*) as total_orders FROM transition_data GROUP BY DATE(ORDER_PURCHASE_TIMESTAMP);\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Add a PRIMARY KEY constraint to the ORDER_ID column in the transition_data table.\",\n",
        "    \"completion\": \"ALTER TABLE transition_data ADD PRIMARY KEY (ORDER_ID);\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Find the latest ORDER_PURCHASE_TIMESTAMP for each ORDER_STATUS in the transition_data table.\",\n",
        "    \"completion\": \"SELECT ORDER_STATUS, MAX(ORDER_PURCHASE_TIMESTAMP) as latest_timestamp FROM transition_data GROUP BY ORDER_STATUS;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Delete all records from the transition_data table where ORDER_STATUS is 'Cancelled'.\",\n",
        "    \"completion\": \"DELETE FROM transition_data WHERE ORDER_STATUS = 'Cancelled';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Alter the transition_data table to change the data type of ORDER_ID to BIGINT.\",\n",
        "    \"completion\": \"ALTER TABLE transition_data MODIFY ORDER_ID BIGINT;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a trigger on the transition_data table to automatically fill ORDER_APPROVED_AT when ORDER_STATUS changes to 'Approved'.\",\n",
        "    \"completion\": \"CREATE TRIGGER fill_order_approved_at BEFORE UPDATE ON transition_data FOR EACH ROW BEGIN IF NEW.ORDER_STATUS = 'Approved' THEN SET NEW.ORDER_APPROVED_AT = CURRENT_TIMESTAMP; END IF; END;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Select all orders from the transition_data table where ORDER_DELIVERED_CUSTOMER_DATE is later than ORDER_ESTIMATED_DELIVERY_DATE.\",\n",
        "    \"completion\": \"SELECT * FROM transition_data WHERE ORDER_DELIVERED_CUSTOMER_DATE > ORDER_ESTIMATED_DELIVERY_DATE;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Update the ORDER_STATUS to 'Delivered' for all orders in the transition_data table where ORDER_DELIVERED_CUSTOMER_DATE is not null.\",\n",
        "    \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'Delivered' WHERE ORDER_DELIVERED_CUSTOMER_DATE IS NOT NULL;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a backup of the transition_data table named transition_data_backup.\",\n",
        "    \"completion\": \"CREATE TABLE transition_data_backup AS SELECT * FROM transition_data;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"List the top 5 customers by the number of orders from the transition_data table.\",\n",
        "    \"completion\": \"SELECT CUSTOMER_ID, COUNT(*) as order_count FROM transition_data GROUP BY CUSTOMER_ID ORDER BY order_count DESC LIMIT 5;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Write a query to retrieve the total number of orders for each ORDER_STATUS from the transition_data table.\",\n",
        "    \"completion\": \"SELECT ORDER_STATUS, COUNT(*) as total_orders FROM transition_data GROUP BY ORDER_STATUS;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a view named customer_order_summary that shows total orders and total amount for each customer.\",\n",
        "    \"completion\": \"CREATE VIEW customer_order_summary AS SELECT CUSTOMER_ID, COUNT(*) as total_orders, SUM(order_amount) as total_amount FROM transition_data GROUP BY CUSTOMER_ID;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Revoke all privileges on the transition_data table from the user role 'guest'.\",\n",
        "    \"completion\": \"REVOKE ALL PRIVILEGES ON transition_data FROM 'guest';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Alter the ORDER_PURCHASE_TIMESTAMP column in the transition_data table to include a time zone.\",\n",
        "    \"completion\": \"ALTER TABLE transition_data ALTER COLUMN ORDER_PURCHASE_TIMESTAMP TYPE TIMESTAMP WITH TIME ZONE;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Find the average order amount for each ORDER_STATUS in the transition_data table.\",\n",
        "    \"completion\": \"SELECT ORDER_STATUS, AVG(order_amount) as avg_amount FROM transition_data GROUP BY ORDER_STATUS;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Insert multiple new orders into the transition_data table.\",\n",
        "    \"completion\": \"INSERT INTO transition_data (ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP) VALUES ('123', '1140337857102', 'c3acec70-0bfd-11ee-a611-fa163e638ddc', 'EMAIL_SEND', '2023-06-29T19:31:00.000Z'), ('123', '1140337857103', 'c3acec71-0bfd-11ee-a611-fa163e638ddc', 'EMAIL_SEND', '2023-06-30T19:31:00.000Z');\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a function to calculate the number of days between ORDER_PURCHASE_TIMESTAMP and ORDER_DELIVERED_CUSTOMER_DATE.\",\n",
        "    \"completion\": \"CREATE FUNCTION calculate_delivery_days(order_id BIGINT) RETURNS INT BEGIN DECLARE days INT; SELECT DATEDIFF(ORDER_DELIVERED_CUSTOMER_DATE, ORDER_PURCHASE_TIMESTAMP) INTO days FROM transition_data WHERE ORDER_ID = order_id; RETURN days; END;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Write a query to lock the transition_data table for exclusive access.\",\n",
        "    \"completion\": \"LOCK TABLE transition_data IN EXCLUSIVE MODE;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Select orders from the transition_data table where the ORDER_STATUS is either 'Pending' or 'Shipped'.\",\n",
        "    \"completion\": \"SELECT * FROM transition_data WHERE ORDER_STATUS IN ('Pending', 'Shipped');\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Update the ORDER_STATUS to 'Shipped' for all orders where ORDER_DELIVERED_CARRIER_DATE is not null.\",\n",
        "    \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'Shipped' WHERE ORDER_DELIVERED_CARRIER_DATE IS NOT NULL;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Delete orders from the transition_data table where ORDER_PURCHASE_TIMESTAMP is older than 1 year.\",\n",
        "    \"completion\": \"DELETE FROM transition_data WHERE ORDER_PURCHASE_TIMESTAMP < CURRENT_DATE - INTERVAL '1 year';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a view named 'recent_orders' to display orders from the last 30 days in the transition_data table.\",\n",
        "    \"completion\": \"CREATE VIEW recent_orders AS SELECT * FROM transition_data WHERE ORDER_PURCHASE_TIMESTAMP >= CURRENT_DATE - INTERVAL '30 days';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"List the number of orders for each ORDER_STATUS, including those without any orders.\",\n",
        "    \"completion\": \"SELECT ORDER_STATUS, COUNT(*) as order_count FROM transition_data GROUP BY ORDER_STATUS WITH ROLLUP;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a trigger to automatically update ORDER_STATUS to 'Delayed' when ORDER_ESTIMATED_DELIVERY_DATE is passed.\",\n",
        "    \"completion\": \"CREATE TRIGGER check_order_delay BEFORE UPDATE ON transition_data FOR EACH ROW BEGIN IF NEW.ORDER_ESTIMATED_DELIVERY_DATE < CURRENT_DATE AND NEW.ORDER_STATUS = 'Pending' THEN SET NEW.ORDER_STATUS = 'Delayed'; END IF; END;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Write a query to change the ORDER_STATUS to 'Completed' for orders where ORDER_DELIVERED_CUSTOMER_DATE is exactly 7 days ago.\",\n",
        "    \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'Completed' WHERE ORDER_DELIVERED_CUSTOMER_DATE = CURRENT_DATE - INTERVAL '7 days';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Grant the 'select' and 'update' privileges on the transition_data table to the user 'data_analyst'.\",\n",
        "    \"completion\": \"GRANT SELECT, UPDATE ON transition_data TO 'data_analyst';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Alter the transition_data table to set the default value of ORDER_STATUS to 'Pending'.\",\n",
        "    \"completion\": \"ALTER TABLE transition_data ALTER COLUMN ORDER_STATUS SET DEFAULT 'Pending';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create an index on the ORDER_STATUS and ORDER_DELIVERED_CUSTOMER_DATE columns in the transition_data table.\",\n",
        "    \"completion\": \"CREATE INDEX idx_status_delivery_date ON transition_data(ORDER_STATUS, ORDER_DELIVERED_CUSTOMER_DATE);\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Update the ORDER_STATUS to 'Processing' for all 'Pending' orders made in the last 24 hours.\",\n",
        "    \"completion\": \"UPDATE transition_data SET ORDER_STATUS = 'Processing' WHERE ORDER_STATUS = 'Pending' AND ORDER_PURCHASE_TIMESTAMP >= CURRENT_DATE - INTERVAL '1 day';\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Select orders from the transition_data table that were placed within the last week and have not been approved yet.\",\n",
        "    \"completion\": \"SELECT * FROM transition_data WHERE ORDER_PURCHASE_TIMESTAMP >= CURRENT_DATE - INTERVAL '7 days' AND ORDER_APPROVED_AT IS NULL;\\n\"\n",
        "},\n",
        "{\n",
        "    \"prompt\": \"Create a stored procedure named 'CancelOrder' to set the ORDER_STATUS to 'Cancelled' for a given ORDER_ID in the transition_data table.\",\n",
        "    \"completion\": \"CREATE PROCEDURE CancelOrder(IN p_order_id VARCHAR(255)) BEGIN UPDATE transition_data SET ORDER_STATUS = 'Cancelled' WHERE ORDER_ID = p_order_id; END;\\n\"\n",
        "}\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# Print the total number of entries in the 'data' dataset\n",
        "print(\"Total number of data entries:\", len(data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90bRkqHPCOZA"
      },
      "outputs": [],
      "source": [
        "# Additional SQL Scenarios for Rename and Delete Operations\n",
        "rename_and_delete_data = [\n",
        "    {\n",
        "        \"prompt\": \"Rename the 'order_amount' column to 'total_order_amount' in the transition_data table.\",\n",
        "        \"completion\": \"ALTER TABLE transition_data RENAME COLUMN order_amount TO total_order_amount;\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"delete the 'extra_column' from the 'transition_data' table\",\n",
        "        \"completion\": \"ALTER TABLE transition_data DROP COLUMN extra_column;\\n\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Adding new scenarios to the existing 'data' list\n",
        "data.extend(rename_and_delete_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXbbYmUurMnG"
      },
      "outputs": [],
      "source": [
        "#12/4 update\n",
        "\n",
        "# Define the Postgresss sql for transition_data\n",
        "table_context = \"### Postgres SQL tables, with their properties:\\n#\\n# transition_data(ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP, ORDER_APPROVED_AT, ORDER_DELIVERED_CARRIER_DATE, ORDER_DELIVERED_CUSTOMER_DATE, ORDER_ESTIMATED_DELIVERY_DATE)\\n#\\n###\\n\\n\"\n",
        "\n",
        "# Iterate through each entry in the 'data' list and prepend the Postgresss sql\n",
        "for entry in data:\n",
        "    entry['prompt'] = table_context + entry['prompt']\n",
        "\n",
        "# Now, each prompt in the 'data' list will include the specified Postgresss sql\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccNrd_FJzvpY"
      },
      "outputs": [],
      "source": [
        "# 12/1/2023 update\n",
        "\n",
        "# Delete and Rename SQL Test Cases\n",
        "#delete_rename_data = [\n",
        "#    {\n",
        "#        \"prompt\": \"### Postgres SQL tables, with their properties:\\n#\\n# transition_data(ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP, ORDER_APPROVED_AT, ORDER_DELIVERED_CARRIER_DATE, ORDER_DELIVERED_CUSTOMER_DATE, ORDER_ESTIMATED_DELIVERY_DATE)\\n#\\n### Write a query to rename the column 'ORDER_DELIVERED_CUSTOMER_DATE' to 'DELIVERY_DATE'\\n\\n\",\n",
        "#        \"completion\": \"ALTER TABLE transition_data RENAME COLUMN ORDER_DELIVERED_CUSTOMER_DATE TO DELIVERY_DATE;\\n\"\n",
        "#    },\n",
        "#    {\n",
        "#        \"prompt\": \"### Postgres SQL tables, with their properties:\\n#\\n# transition_data(ACCOUNT_ID, ORDER_ID, CUSTOMER_ID, ORDER_STATUS, ORDER_PURCHASE_TIMESTAMP, ORDER_APPROVED_AT, ORDER_DELIVERED_CARRIER_DATE, ORDER_DELIVERED_CUSTOMER_DATE, ORDER_ESTIMATED_DELIVERY_DATE)\\n#\\n### Write a query to delete all records where 'ORDER_STATUS' is 'Cancelled'\\n\\n\",\n",
        "#        \"completion\": \"DELETE FROM transition_data WHERE ORDER_STATUS = 'Cancelled';\\n\"\n",
        "#    }\n",
        "    # Add more delete and rename prompts and completions if needed\n",
        "#]\n",
        "\n",
        "# Adding new scenarios to the existing 'data' list\n",
        "#data.extend(delete_rename_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hkqOi57y-T7",
        "outputId": "fbc413d5-da57-4a9f-da5b-7e390a4777a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 duplicates.\n",
            "Total number of unique data entries: 52\n"
          ]
        }
      ],
      "source": [
        "# Function to check for duplicates\n",
        "def check_duplicates(data):\n",
        "    seen = set()\n",
        "    duplicates = []\n",
        "    for entry in data:\n",
        "        prompt_completion_pair = (entry['prompt'], entry['completion'])\n",
        "        if prompt_completion_pair in seen:\n",
        "            duplicates.append(entry)\n",
        "        else:\n",
        "            seen.add(prompt_completion_pair)\n",
        "    return duplicates\n",
        "\n",
        "# Find duplicates in the dataset\n",
        "duplicates = check_duplicates(data)\n",
        "\n",
        "# Print out the duplicates\n",
        "print(f\"Found {len(duplicates)} duplicates.\")\n",
        "for i, dup in enumerate(duplicates, 1):\n",
        "    print(f\"Duplicate {i}: Prompt: {dup['prompt']}, Completion: {dup['completion']}\")\n",
        "\n",
        "# Print the total number of unique entries\n",
        "print(\"Total number of unique data entries:\", len(data) - len(duplicates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBxM6bWSGlS"
      },
      "source": [
        "Adding Table Creation Context:\n",
        "\n",
        "- Define a command that describes the creation of the transition_data table.\n",
        "- Prepend this table creation command to each prompt for better context understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld-bIctj3tfA"
      },
      "outputs": [],
      "source": [
        "#Add Table Creation Command to Prompts:\n",
        "\n",
        "#Pre-appends a standard table creation command to each prompt for context.\n",
        "#table_creation_command = \"The table has been created using the following command: CREATE TABLE transition_data (ACCOUNT_ID VARCHAR(50), ORDER_ID VARCHAR(255), CUSTOMER_ID VARCHAR(255), ORDER_STATUS VARCHAR(100), ORDER_PURCHASE_TIMESTAMP TIMESTAMP, ORDER_APPROVED_AT TIMESTAMP, ORDER_DELIVERED_CARRIER_DATE TIMESTAMP, ORDER_DELIVERED_CUSTOMER_DATE TIMESTAMP, ORDER_ESTIMATED_DELIVERY_DATE TIMESTAMP)\\n\\n\"\n",
        "\n",
        "# Update prompts with the table creation command\n",
        "#for item in data:\n",
        "#    item['prompt'] = table_creation_command + item['prompt']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQztY6l3Ti0w"
      },
      "source": [
        "Saving Data to a File:\n",
        "\n",
        "- Save all the data entries to a JSONL file (training_data.jsonl)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFp2VQKAifT8"
      },
      "outputs": [],
      "source": [
        "# Save the data to a JSONL file\n",
        "with open('training_data.jsonl', 'w') as file:\n",
        "    for entry in data:\n",
        "        json.dump(entry, file)\n",
        "        file.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2foaFA-KT1Xw"
      },
      "source": [
        "Uploading the File for Fine-Tuning:\n",
        "\n",
        "- Define the headers with the API key for authorization.\n",
        "- Define the endpoint URL for file upload and prepare the file data.\n",
        "- Make the file upload request and check if it was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIikZgHKijRD"
      },
      "outputs": [],
      "source": [
        "# Define the headers for authorization\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {api_key}'\n",
        "}\n",
        "\n",
        "# Define the endpoint URL for file upload\n",
        "file_upload_url = 'https://api.openai.com/v1/files'\n",
        "\n",
        "# Prepare the file data to send\n",
        "files_to_upload = {\n",
        "    'file': ('training_data.jsonl', open('training_data.jsonl', 'rb')),\n",
        "    'purpose': (None, 'fine-tune')\n",
        "}\n",
        "\n",
        "# Make the file upload request\n",
        "file_response = requests.post(file_upload_url, headers=headers, files=files_to_upload)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htSLx9-HUEkg"
      },
      "source": [
        "Starting the Fine-Tuning Process:\n",
        "\n",
        "- Define the fine-tune endpoint URL and prepare the fine-tune parameters.\n",
        "- Make the fine-tune request and check if it was successful.\n",
        "- Monitor the fine-tuning job's status in a loop, checking every 60 seconds.\n",
        "- Check if the fine-tuning job succeeded and get the model ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnlB7ovvimkM",
        "outputId": "71555472-5524-4080-99b9-9581a0f0a7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File uploaded successfully with ID: file-7U4W9TmqRDuhNnQ9zZWQwvTw\n",
            "Fine-tuning started with ID: ft-mTp47EDQWYMwOqMqHTD0sX8n\n",
            "Fine-tune job status: pending\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: running\n",
            "Fine-tune job status: succeeded\n",
            "Fine-tuned model ID: davinci:ft-siriusmindshare-2023-12-22-19-51-04\n"
          ]
        }
      ],
      "source": [
        "# Check if the file upload was successful\n",
        "if file_response.status_code == 200:\n",
        "    file_id = file_response.json()['id']\n",
        "    print(f\"File uploaded successfully with ID: {file_id}\")\n",
        "\n",
        "    # Define the endpoint URL for starting the fine-tune\n",
        "    fine_tune_url = 'https://api.openai.com/v1/fine-tunes'\n",
        "\n",
        "    # Prepare the fine-tune parameters\n",
        "    fine_tune_data = {\n",
        "        'training_file': file_id,\n",
        "        'model': 'davinci',\n",
        "        'n_epochs': 10,\n",
        "        'batch_size': 4\n",
        "    }\n",
        "\n",
        "    # Make the fine-tune request\n",
        "    fine_tune_response = requests.post(fine_tune_url, headers=headers, json=fine_tune_data)\n",
        "\n",
        "    # Check if the fine-tune request was successful\n",
        "    if fine_tune_response.status_code == 200:\n",
        "        fine_tune_id = fine_tune_response.json()['id']\n",
        "        print(f\"Fine-tuning started with ID: {fine_tune_id}\")\n",
        "\n",
        "        # Define the endpoint URL for checking fine-tune status\n",
        "        fine_tune_status_url = f'https://api.openai.com/v1/fine-tunes/{fine_tune_id}'\n",
        "\n",
        "        # Monitor the fine-tuning job\n",
        "        while True:\n",
        "            status_response = requests.get(fine_tune_status_url, headers=headers)\n",
        "\n",
        "            # Check if the request was successful\n",
        "            if status_response.status_code == 200:\n",
        "                status = status_response.json()['status']\n",
        "                print(f\"Fine-tune job status: {status}\")\n",
        "\n",
        "                # Break out of the loop if the job is no longer running\n",
        "                if status not in ['running', 'pending']:\n",
        "                    break\n",
        "            else:\n",
        "                print(\"Failed to retrieve fine-tuning status:\", status_response.json())\n",
        "                break\n",
        "\n",
        "            time.sleep(60)  # Wait for 60 seconds before checking again\n",
        "\n",
        "        # Check if the fine-tuning job succeeded\n",
        "        if status == 'succeeded':\n",
        "            print(f\"Fine-tuned model ID: {status_response.json()['fine_tuned_model']}\")\n",
        "            model_id = status_response.json()['fine_tuned_model']\n",
        "        else:\n",
        "            print(f\"Fine-tuning failed or was cancelled: {status}\")\n",
        "            model_id = None\n",
        "    else:\n",
        "        print(\"Failed to start fine-tuning:\", fine_tune_response.json())\n",
        "else:\n",
        "    print(\"File upload failed:\", file_response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIlnSJSUOaY"
      },
      "source": [
        "Using the Fine-Tuned Model:\n",
        "\n",
        "- Use the fine-tuned model to create completions based on given prompts.\n",
        "- Define a function validate_sql for a simple validation of SQL queries.\n",
        "- Define a function generate_sql_completions to generate SQL completions.\n",
        "- Generate, score, and select the best SQL proposals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cIMP_m1irAh",
        "outputId": "086e4818-88db-4851-e9a4-1cdf62c15e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DELETE FROM transition_data WHERE extra_column = 'New Name';\n",
            "\n",
            "Step 7.0: Create a Table to Hold Dashboard Data\n",
            "\n",
            "Create a table named 'my_dashboard_data' and insert the following data\n"
          ]
        }
      ],
      "source": [
        "# Use the fine-tuned model\n",
        "if model_id:\n",
        "    # Here you can use the fine-tuned model to create completions.\n",
        "    # Replace the following prompt with whatever prompt you'd like to use:\n",
        "    prompt = \"Write a query to delete the 'extra_column' from the 'transition_data' table.\"\n",
        "    response = requests.post(\n",
        "        'https://api.openai.com/v1/completions',\n",
        "        headers=headers,\n",
        "        json={\n",
        "            'model': model_id,\n",
        "            'prompt': prompt,\n",
        "            'max_tokens': 50\n",
        "        }\n",
        "    )\n",
        "    if response.status_code == 200:\n",
        "        print(response.json()['choices'][0]['text'].strip())\n",
        "    else:\n",
        "        print(\"Failed to generate completion:\", response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHS8zRF0Sx7c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to validate and score SQL queries\n",
        "def validate_sql(sql_query):\n",
        "    # Implement your validation logic here\n",
        "    # For now, let's just return a dummy score\n",
        "\n",
        "    return len(sql_query)  # Example: shorter queries score higher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygXTwNRIS0vu"
      },
      "outputs": [],
      "source": [
        "# Function to generate SQL completions\n",
        "def generate_sql_completions(prompt, n=10):\n",
        "    responses = []\n",
        "    for _ in range(n):\n",
        "        response = requests.post(\n",
        "            'https://api.openai.com/v1/completions',\n",
        "            headers=headers,\n",
        "            json={\n",
        "                'model': model_id,\n",
        "                'prompt': prompt,\n",
        "                'max_tokens': 50\n",
        "            }\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            sql_query = response.json()['choices'][0]['text'].strip()\n",
        "            responses.append(sql_query)\n",
        "        else:\n",
        "            print(\"Failed to generate completion:\", response.json())\n",
        "    return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2qM28H4S3Wg"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "prompt = \"Write a query to delete the 'extra_column' from the 'transition_data' table.\"\n",
        "generated_responses = generate_sql_completions(prompt, n=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJMjPA2LS5Gd"
      },
      "outputs": [],
      "source": [
        "# Score and select the best 3 SQL proposals\n",
        "scored_responses = [(sql, validate_sql(sql)) for sql in generated_responses]\n",
        "top_3_responses = sorted(scored_responses, key=lambda x: x[1], reverse=True)[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Toc0GosTM7i",
        "outputId": "7df5cd0b-3030-4588-f302-0244fd482a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 SQL Query: DELETE FROM transition_data WHERE extra_column = 'First Name';\n",
            "\n",
            "Step 5 – Delete rows based on condition using TRUNCATE TABLE statement\n",
            "\n",
            "Use the TRUNCATE TABLE statement to delete all rows from the ' (Score: 198)\n",
            "Top 2 SQL Query: BEGIN DELETE FROM transition_data WHERE extra_column = 'X'; END;\n",
            "\n",
            "Are you using PostgreSQL in your application? Check out our handy migration guide for Postgres!\n",
            "\n",
            "Conclusion\n",
            "\n",
            "In this article, we (Score: 194)\n",
            "Top 3 SQL Query: delete FROM transition_data WHERE extra_column = 'abcd';\n",
            "\n",
            "Let's see the data in the 'transition_data' table after the deletion.SELECT * FROM transition_data;\n",
            "\n",
            "Column Family: transition_data COL (Score: 193)\n"
          ]
        }
      ],
      "source": [
        "# Print the top 3 responses\n",
        "for i, (sql, score) in enumerate(top_3_responses, 1):\n",
        "    print(f\"Top {i} SQL Query: {sql} (Score: {score})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJN-_wvRgP9T"
      },
      "source": [
        "Add QA Verification Function\n",
        "\n",
        "After all the current steps (fine-tuning, generating SQL completions, etc.), we'll add a function to perform QA verification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40zGZm8XTOyi",
        "outputId": "892660c0-7975-43d4-ba57-4ef163475fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA verification completed. All ORDER_STATUS entries have been updated.\n"
          ]
        }
      ],
      "source": [
        "# QA Verification Function\n",
        "def qa_verification(data_list):\n",
        "    for entry in data_list:\n",
        "        # Example QA Step: Append '_verified' to ORDER_STATUS in SQL completion\n",
        "        if 'ORDER_STATUS' in entry['completion']:\n",
        "            entry['completion'] = entry['completion'].replace(\"ORDER_STATUS\", \"ORDER_STATUS_verified\")\n",
        "    print(\"QA verification completed. All ORDER_STATUS entries have been updated.\")\n",
        "\n",
        "# Apply the QA Verification\n",
        "qa_verification(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzy1wTZXgZSF"
      },
      "outputs": [],
      "source": [
        "# Save the updated data to a JSONL file\n",
        "with open('training_data_updated.jsonl', 'w') as file:\n",
        "    for entry in data:\n",
        "        json.dump(entry, file)\n",
        "        file.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTouqG4Vo-eB"
      },
      "outputs": [],
      "source": [
        "# SQL Query Quality Assurance\n",
        "# This section is dedicated to verifying the quality of generated SQL queries.\n",
        "\n",
        "# Function to Perform Basic SQL Query Validation\n",
        "def validate_sql_query(sql_query):\n",
        "    # This function can be expanded with complex logic to validate SQL queries\n",
        "    # For demonstration, a basic keyword check is implemented\n",
        "    essential_keywords = ['SELECT', 'UPDATE', 'DELETE', 'INSERT', 'ALTER', 'CREATE']\n",
        "    return any(keyword in sql_query.upper() for keyword in essential_keywords)\n",
        "\n",
        "# Function to Apply Quality Assurance Checks\n",
        "def qa_sql_queries(queries):\n",
        "    validated_queries = []\n",
        "    for query in queries:\n",
        "        if validate_sql_query(query):\n",
        "            validated_queries.append(query)\n",
        "    return validated_queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHHyKbGspyrB",
        "outputId": "e57c4655-d3ff-4595-c8bb-1ddcdb186394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified SQL Queries:\n",
            "BEGIN DELETE FROM transition_data WHERE extra_column = 'X'; END;\n",
            "\n",
            "Are you using PostgreSQL in your application? Check out our handy migration guide for Postgres!\n",
            "\n",
            "Conclusion\n",
            "\n",
            "In this article, we\n",
            "DELETE FROM transition_data WHERE extra_column = 'some_data';\n",
            "\n",
            "MODIFY Query MODIFY is used to modify a database object such as a table, view or sequence.Syntax: MODIFY table_\n",
            "DELETE FROM transition_data WHERE extra_column = 'ADDED TO CART'; SELECT * FROM transition_data;\n",
            "\n",
            "Step 5: Insert 2 records into COLOR_REF WITH_DATE table and SELECT them.INSERT\n",
            "delete FROM transition_data WHERE extra_column = 'abcd';\n",
            "\n",
            "Let's see the data in the 'transition_data' table after the deletion.SELECT * FROM transition_data;\n",
            "\n",
            "Column Family: transition_data COL\n",
            "DELETE FROM transition_data WHERE COLUMN_NAME = 'extra_column';\n",
            "\n",
            "Step 9: Delete the packages.DELETE FROM package_data WHERE ID = '11358497677757695727';\n",
            "DELETE FROM transition_data WHERE extra_column = 'First Name';\n",
            "\n",
            "Step 5 – Delete rows based on condition using TRUNCATE TABLE statement\n",
            "\n",
            "Use the TRUNCATE TABLE statement to delete all rows from the '\n",
            "DELETE FROM transition_data WHERE extra_column = 'extraval';\n",
            "\n",
            "SELECT * FROM transition_data;\n",
            "\n",
            "What Happened?\n",
            "\n",
            "Demo of filtering data using WHERE clause\n",
            "\n",
            "Additional Readings\n",
            "\n",
            "Summary\n",
            "DELETE FROM transition_data WHERE extra_column = 'No extra column value';\n",
            "\n",
            "Step 6 – INSERT\n",
            "\n",
            "Write a query to INSERT new data into the 'transition_data' table.INSERT INTO transition_\n",
            "DELETE FROM transition_data WHERE column_name = 'extra_column';\n",
            "\n",
            "STEP 5: Display the data in the 'transition_data' table after deleting the 'extra_column'.SELECT * FROM transition_data;\n",
            "DELETE FROM transition_data WHERE extra_column = 'JavaEE7';\n",
            "\n",
            "Execute the query to verify the deletion:\n",
            "\n",
            "ALTER TABLE transition_data DROP COLUMN extra_column;\n",
            "\n",
            "Verify the\n"
          ]
        }
      ],
      "source": [
        "# Applying QA to Generated SQL Queries\n",
        "verified_sql_queries = qa_sql_queries(generated_responses)\n",
        "\n",
        "# Printing Verified SQL Queries\n",
        "print(\"Verified SQL Queries:\")\n",
        "for query in verified_sql_queries:\n",
        "    print(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cLK1hGRp4_i",
        "outputId": "bdc2fb97-039b-4623-8ea7-46ce8eb74a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quality Assurance Check Completed. Verified queries are ready for use.\n"
          ]
        }
      ],
      "source": [
        "# End of Quality Assurance Process\n",
        "print(\"Quality Assurance Check Completed. Verified queries are ready for use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRwWs_CWp8O9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zf2zD6_24JV7",
        "VcpxVjxYlgqW",
        "n4kK9ui6l18g",
        "52PV-Qs0rMua",
        "mUTzaNKzm2Pg",
        "cTytr-foDplk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
